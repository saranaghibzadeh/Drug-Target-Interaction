{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 44254,
     "status": "ok",
     "timestamp": 1740418642944,
     "user": {
      "displayName": "Ali Ghanbari",
      "userId": "17860187318684898656"
     },
     "user_tz": -210
    },
    "id": "OAkqVkPW1Ysc",
    "outputId": "1c97f8ae-2745-414c-d595-e4bdbd045775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive not mounted, so nothing to flush and unmount.\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.flush_and_unmount()\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkUxKQd2-c5M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)  \n",
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_columns', None)\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "import random\n",
    "import math\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,ConfusionMatrixDisplay, precision_recall_curve\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zt34WpXC3tEm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mouvLAhLsiE0"
   },
   "outputs": [],
   "source": [
    "Allpath='/content/drive/My Drive/Colab Notebooks/Naghibzadeh/'\n",
    "DataNum=4\n",
    "if DataNum==1:\n",
    "  DBname='df_final_NR.csv'\n",
    "elif DataNum==2:\n",
    "    DBname='df_final_IC.csv'\n",
    "elif DataNum==3:\n",
    "    DBname='df_final_GPCR.csv'\n",
    "elif DataNum==4:\n",
    "    DBname='df_final_EN.csv'\n",
    "NewAdd=Allpath+'NewResult/'+DBname[:-4]+'/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1k3FWGDo6n2"
   },
   "outputs": [],
   "source": [
    "nPCA=100\n",
    "PCAFlag=1\n",
    "NormFlag=1\n",
    "scaler = MinMaxScaler()\n",
    "# *** groups ***\n",
    "# Read drug-target interactions\n",
    "df_inter = pd.read_csv(Allpath+DBname)\n",
    "\n",
    "\n",
    "df_inter.drop(df_inter.columns[df_inter.columns.str.contains('unnamed', case=False)], axis=1, inplace=True)\n",
    "#print(df_inter.head())\n",
    "\n",
    "count_label_0 = sum(df_inter['label'] == 0)\n",
    "count_label_1 = sum(df_inter['label'] == 1)\n",
    "\n",
    "\n",
    "# **Splitting samples of label(0) **\n",
    "df_label_0 = df_inter.loc[df_inter['label'] == 0]\n",
    "df_label_1 = df_inter.loc[df_inter['label'] == 1]\n",
    "hd1x = df_label_1.copy()\n",
    "hd1y=df_label_1['label']\n",
    "hd1x.drop(['drug_no', 'protein_no', 'label'], axis=1, inplace=True)\n",
    "#print(hd1y)\n",
    "\n",
    "hd0x = df_label_0.copy()\n",
    "hd0y=df_label_0['label']\n",
    "hd0x.drop(['drug_no', 'protein_no', 'label'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#print(hd0y)\n",
    "\n",
    "hd1x = hd1x.to_numpy()\n",
    "hd1y = hd1y.to_numpy()\n",
    "hd0x = hd0x.to_numpy()\n",
    "hd0y = hd0y.to_numpy()\n",
    "if PCAFlag==1:\n",
    "  XPCANew=np.concatenate([hd0x,hd1x])\n",
    "  pca = PCA(n_components=nPCA)\n",
    "  pca.fit(XPCANew)\n",
    "  XPCANew=pca.transform(XPCANew)\n",
    "  hd0x=XPCANew[:len(hd0x),:]\n",
    "  hd1x=XPCANew[len(hd0x):,:]\n",
    "\n",
    "\n",
    "rand0 = np.random.permutation(hd0x.shape[0])\n",
    "rand1 = np.random.permutation(hd1x.shape[0])\n",
    "index0=math.ceil(0.25*hd0x.shape[0])\n",
    "index1=math.ceil(0.25*hd1x.shape[0])\n",
    "\n",
    "hd0x_test=hd0x[rand0[:index0],:]\n",
    "hd0x=hd0x[rand0[index0+1:],:]\n",
    "hd0y_test = hd0y[rand0[:index0]]\n",
    "hd0y=hd0y[rand0[index0+1:]]\n",
    "\n",
    "hd1x_test=hd1x[rand1[:index1],:]\n",
    "hd1x=hd1x[rand1[index1:],:]\n",
    "hd1y_test = hd1y[rand1[:index1]]\n",
    "hd1y=hd1y[rand1[index1:]]\n",
    "\n",
    "\n",
    "\n",
    "XPCA=np.concatenate([hd0x,hd1x])\n",
    "\n",
    "if NormFlag==1:\n",
    "  scaler.fit(XPCA)\n",
    "  XPCA=scaler.transform(XPCA)\n",
    "hd0x=XPCA[:len(hd0x),:]\n",
    "hd1x=XPCA[len(hd0x):,:]\n",
    "\n",
    "XPCA_test=np.concatenate([hd0x_test,hd1x_test])\n",
    "\n",
    "if NormFlag==1:\n",
    "  XPCA_test=scaler.transform(XPCA_test)\n",
    "hd0x_test=XPCA_test[:len(hd0x_test),:]\n",
    "hd1x_test=XPCA_test[len(hd0x_test):,:]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1sbxVxUQu42V54Yw7YPMSYzW6XwmV1iCu"
    },
    "executionInfo": {
     "elapsed": 28897,
     "status": "ok",
     "timestamp": 1740418722760,
     "user": {
      "displayName": "Ali Ghanbari",
      "userId": "17860187318684898656"
     },
     "user_tz": -210
    },
    "id": "yXVS1aglubhW",
    "outputId": "5d1e34fc-5a7c-4813-f1d4-e646b30c0aa0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xnew=np.concatenate([hd0x,hd1x,hd0x_test,hd1x_test])\n",
    "Ynew=np.concatenate([hd0y,hd1y,hd0y_test,hd1y_test])\n",
    "XAll=np.concatenate([Xnew,Ynew.reshape(-1,1)],axis=1)\n",
    "hd=pd.DataFrame(data=XAll)\n",
    "#hd.head()\n",
    "print('Sample: ', hd.shape[0])\n",
    "print('attributes: ', hd.shape[1] - 1)\n",
    "hd.describe()\n",
    "\n",
    "\n",
    "def draw_histograms(dataframe, features, rows, cols):\n",
    "    num_features = len(features)\n",
    "    max_plots = rows * cols \n",
    "    num_pages = (num_features + max_plots - 1) // max_plots  \n",
    "\n",
    "    for page in range(num_pages):\n",
    "        start_idx = page * max_plots\n",
    "        end_idx = min(start_idx + max_plots, num_features)  \n",
    "        fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "        for i, feature in enumerate(features[start_idx:end_idx]):\n",
    "            ax = fig.add_subplot(rows, cols, i + 1)\n",
    "            dataframe[feature].hist(bins=20, ax=ax, facecolor='DarkBlue')\n",
    "            ax.set_title(feature, color='DarkRed')\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "if 15 not in hd.columns:\n",
    "    raise KeyError(\"Column 15 does not exist in the DataFrame. Please check your column indices.\")\n",
    "\n",
    "\n",
    "num_features = len(hd.columns)\n",
    "max_features = 18  \n",
    "\n",
    "\n",
    "draw_histograms(hd, hd.columns[:min(num_features, max_features)], rows=6, cols=3)\n",
    "\n",
    "\n",
    "numeric_features = hd.select_dtypes(include=[np.number])\n",
    "corr = numeric_features.corr(method='spearman')\n",
    "plt.figure(figsize=(max(10, corr.shape[1] * 1.5), max(10, corr.shape[0] * 1.5)))\n",
    "sns.heatmap(corr, square=True, cbar=True, annot=True, fmt=\".2f\")\n",
    "plt.title(\"Spearman Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "chdc = Counter(hd[15])\n",
    "print('Interaction: ', chdc[1])\n",
    "print('Non-Interaction: ', chdc[0])\n",
    "plt.bar(list(chdc.keys()), list(chdc.values()), color=['g', 'r'])\n",
    "\n",
    "\n",
    "print(f\"Unique values in column 15: {list(chdc.keys())}\")\n",
    "\n",
    "\n",
    "if len(chdc.keys()) == 2:  \n",
    "    plt.bar(list(chdc.keys()), list(chdc.values()), color=['g', 'r'])\n",
    "    plt.xticks(list(chdc.keys()), ['No', 'Yes'])\n",
    "    plt.title('Interaction and Non-Interaction')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plt.title('Interaction and Non-Interaction')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LLhfGhjUDgW"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1740418732569,
     "user": {
      "displayName": "Ali Ghanbari",
      "userId": "17860187318684898656"
     },
     "user_tz": -210
    },
    "id": "h2tvnsI59P66",
    "outputId": "cbd3c027-68ae-4bcc-e3f0-2d5862a639fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "575\n",
      "596\n",
      "(4680, 100)\n",
      "(1171, 100)\n",
      "(4680,)\n",
      "(1171,)\n",
      "[0 0 1 1 1 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X=np.concatenate([hd0x,hd1x,hd0x_test,hd1x_test])\n",
    "Y=np.concatenate([hd0y,hd1y,hd0y_test,hd1y_test])\n",
    "from sklearn.model_selection import train_test_split\n",
    "newtrain_X, newtest_X, newtrain_Y, newtest_Y = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "X_rsam=np.concatenate([newtrain_X,newtest_X])\n",
    "Y_rsam=np.concatenate([newtrain_Y,newtest_Y])\n",
    "cc = Counter(newtest_Y)\n",
    "print(cc[1])\n",
    "print(cc[0])\n",
    "\n",
    "print(newtrain_X.shape)\n",
    "print(newtest_X.shape)\n",
    "print(newtrain_Y.shape)\n",
    "print(newtest_Y.shape)\n",
    "\n",
    "print(newtrain_Y[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPRAL3b1uNIY"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import CondensedNearestNeighbour\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
    "from imblearn.under_sampling import AllKNN\n",
    "from imblearn.under_sampling import InstanceHardnessThreshold\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import KMeansSMOTE\n",
    "from imblearn.over_sampling import SVMSMOTE\n",
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.combine import SMOTETomek\n",
    "OLD_reSampler=[[ClusterCentroids(estimator=MiniBatchKMeans(n_init=1, random_state=0), random_state=42),\n",
    "                    CondensedNearestNeighbour(random_state=42),\n",
    "                    EditedNearestNeighbours(),\n",
    "                    RepeatedEditedNearestNeighbours(),\n",
    "                    #AllKNN(),\n",
    "                    #InstanceHardnessThreshold(random_state=42),\n",
    "                    NeighbourhoodCleaningRule(),\n",
    "                    OneSidedSelection(random_state=42),\n",
    "                    RandomUnderSampler(random_state=42),\n",
    "                    TomekLinks(),\n",
    "                    ],[ RandomOverSampler(random_state=42),\n",
    "                    SMOTE(random_state=42),\n",
    "                    SMOTENC(random_state=42, categorical_features=[18, 19]),\n",
    "                    SMOTEN(random_state=0),\n",
    "                    #ADASYN(random_state=42),\n",
    "                    BorderlineSMOTE(random_state=42),\n",
    "#                    KMeansSMOTE(kmeans_estimator=MiniBatchKMeans(n_init=1, random_state=0), random_state=42),\n",
    "                    SVMSMOTE(random_state=42)\n",
    "                    ], [#SMOTEENN(random_state=42),\n",
    "                  SMOTETomek(random_state=42),\n",
    "                  ]]\n",
    "\n",
    "OLD_reSamplerName=[['ClusterCentroids',\n",
    "                    'ConNearNeigh',\n",
    "                    'EditNearNeighb',\n",
    "                    'ReEditNearNeighb',\n",
    "                    #'AllKNN',\n",
    "                    #'InstHardThre',\n",
    "                    'NeighbCleanRule',\n",
    "                    'OneSidedSel',\n",
    "                    'RandUnderSam',\n",
    "                    'TomekLinks',\n",
    "                    ],[ 'RandOverSampler',\n",
    "                    'SMOTE',\n",
    "                    'SMOTENC',\n",
    "                    'SMOTEN',\n",
    "                    #'ADASYN',\n",
    "                    'BorderlineSMOTE',\n",
    "                    #'KMeansSMOTE',\n",
    "                    'SVMSMOTE'\n",
    "                    ], [#'SMOTEENN',\n",
    "                  'SMOTETomek',\n",
    "                  ]]\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from numpy import array, concatenate, mean, std, reshape, zeros, ones, arange\n",
    "import numpy as np\n",
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    #\"RBF SVM\",\n",
    "    #\"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "    \"LDA\",\n",
    "    \"EEC\",\n",
    "    \"RSC\",\n",
    "    \"BBC\",\n",
    "    \"BRFC\"\n",
    "\n",
    "]\n",
    "base_model11 = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=10),\n",
    "    #SVC(gamma=2, C=1, random_state=42,probability=True),\n",
    "    #GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42),\n",
    "    DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    RandomForestClassifier(\n",
    "        max_depth=50, n_estimators=100),\n",
    "    MLPClassifier(alpha=1, max_iter=1000, random_state=42),\n",
    "    AdaBoostClassifier(n_estimators=10, random_state=42),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LinearDiscriminantAnalysis(),\n",
    "    EasyEnsembleClassifier(random_state=42),\n",
    "    RUSBoostClassifier(n_estimators=10),  # Changed n_estimatorsrandom_state=0),\n",
    "    BalancedBaggingClassifier(random_state=42),\n",
    "    BalancedRandomForestClassifier(sampling_strategy=\"all\", replacement=True, max_depth=2, random_state=0, bootstrap=False)\n",
    "]\n",
    "def evaluate_model(clf, X, y):\n",
    "    pred = clf.predict(X) #predicted classes\n",
    "    accuracy = accuracy_score(pred,y) # calculate accuracy\n",
    "    fpr, tpr, _ = roc_curve(y, clf.predict(X)) # roc_curve\n",
    "    auc_value = auc(fpr,tpr) # auc_value\n",
    "    report = classification_report(y, pred, labels=[0,1], output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_df = report_df.reset_index()\n",
    "    model_eval  = report_df[report_df['index'].str.contains('1')][['precision','recall','f1-score']]\n",
    "    model_eval['accuracy']  = list(report_df[report_df['index'].str.contains('accuracy')]['support'])\n",
    "    model_eval['ROC']  = auc_value\n",
    "    cf_matrix = confusion_matrix(y, pred)\n",
    "    return model_eval,pred,y, cf_matrix\n",
    "\n",
    "def model_eval_data(clf, X_train, y_train,\n",
    "                         X_test, y_test,\n",
    "                         model_eval_train,\n",
    "                         model_eval_test,\n",
    "                         model_eval_train_y,\n",
    "                         model_eval_test_y,\n",
    "                         SaveStep1,\n",
    "                         Name=None):\n",
    "\n",
    "    temp_eval_train,pred_train1,y_train1, cf_matrix_train = evaluate_model(clf, X_train, y_train)\n",
    "    temp_eval_test,pred_test1,y_test1, cf_matrix_test = evaluate_model(clf, X_test, y_test)\n",
    "\n",
    "    temp_train_y=pd.DataFrame()\n",
    "    temp_test_y=pd.DataFrame()\n",
    "    if SaveStep1==1:\n",
    "      temp_train_y['Value']=[y_train1]\n",
    "      temp_test_y['Value']=[y_test1]\n",
    "      temp_train_y.index = ['real']\n",
    "      temp_test_y.index = ['real']\n",
    "      try:\n",
    "        model_eval_train_y = pd.concat([model_eval_train_y, temp_train_y])\n",
    "        model_eval_test_y = pd.concat([model_eval_test_y, temp_test_y])\n",
    "        #model_eval_train = model_eval_train.append(temp_eval_train)\n",
    "        #model_eval_test = model_eval_test.append(temp_eval_test)\n",
    "      except:\n",
    "        model_eval_train_y = temp_train_y\n",
    "        model_eval_test_y = temp_test_y\n",
    "\n",
    "    temp_eval_train.index = [Name]\n",
    "    temp_eval_test.index = [Name]\n",
    "    temp_train_y['Value']=[pred_train1]\n",
    "    temp_test_y['Value']=[pred_test1]\n",
    "    temp_train_y.index = [Name]\n",
    "    temp_test_y.index = [Name]\n",
    "\n",
    "    try:\n",
    "        model_eval_train = pd.concat([model_eval_train, temp_eval_train])\n",
    "        model_eval_test = pd.concat([model_eval_test, temp_eval_test])\n",
    "        model_eval_train_y = pd.concat([model_eval_train_y, temp_train_y])\n",
    "        model_eval_test_y = pd.concat([model_eval_test_y, temp_test_y])\n",
    "        #model_eval_train = model_eval_train.append(temp_eval_train)\n",
    "        #model_eval_test = model_eval_test.append(temp_eval_test)\n",
    "    except:\n",
    "        model_eval_train = temp_eval_train\n",
    "        model_eval_test = temp_eval_test\n",
    "        model_eval_train_y = temp_train_y\n",
    "        model_eval_test_y = temp_test_y\n",
    "\n",
    "    return model_eval_train, model_eval_test,model_eval_train_y, model_eval_test_y, cf_matrix_train, cf_matrix_test\n",
    "def make_confusion_matrix_chart(cf_matrix_train, cf_matrix_test,pltname):\n",
    "\n",
    "    #plt.figure()\n",
    "    #plt.title(pltname)\n",
    "    #plt.subplot(121)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cf_matrix_train,\n",
    "                              display_labels=['Not Interaction', 'Interaction'])\n",
    "\n",
    "    disp.plot()\n",
    "    plt.xticks(rotation='horizontal')\n",
    "    plt.yticks(rotation='vertical')\n",
    "    plt.title('Train_'+pltname)\n",
    "    #plt.gca().set_xticklabels([])\n",
    "    plt.savefig(NewAdd+'Train_'+pltname+'.png')\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cf_matrix_test,\n",
    "                              display_labels=['Not Interaction', 'Interaction'])\n",
    "\n",
    "    disp.plot()\n",
    "    plt.xticks(rotation='horizontal')\n",
    "    plt.yticks(rotation='vertical')\n",
    "    plt.title('Test_'+pltname)\n",
    "    #plt.gca().set_xticklabels([])\n",
    "    plt.savefig(NewAdd+'Test_'+pltname+'.png')\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1Ncg8c2HGY5AiiG4Ued6Blq7lMZqdwzWS"
    },
    "id": "4CC0nKJUKcOs",
    "outputId": "f965f4ae-8f8b-464e-b8d8-4f8a3402c384"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "saveStep1=1\n",
    "model_eval_train = pd.DataFrame({},[])\n",
    "model_eval_test = pd.DataFrame({},[])\n",
    "model_eval_train_y = pd.DataFrame({},[])\n",
    "model_eval_test_y = pd.DataFrame({},[])\n",
    "for name_res1, clf_res1 in zip(OLD_reSamplerName, OLD_reSampler):\n",
    "  for name_res2, clf_res2 in zip(name_res1, clf_res1):\n",
    "    print('Resample: ',name_res2)\n",
    "    X_augmented_data, y_augmented_data = clf_res2.fit_resample(X_rsam, Y_rsam)\n",
    "    for name, clf in zip(names, classifiers):\n",
    "      print(name_res2+'-'+name+'-')\n",
    "      '''print(X_augmented_data.shape)\n",
    "      count_ones = np.count_nonzero(y_augmented_data == 1)\n",
    "      count_zeros = np.count_nonzero(y_augmented_data == 0)\n",
    "      print(\"Number of 1s:\", count_ones)\n",
    "      print(\"Number of 0s:\", count_zeros)'''\n",
    "      #start_time = time.time()\n",
    "      clf1 = clf.fit(X_augmented_data, y_augmented_data)\n",
    "      #clf1 =clf.fit(newtest_X, newtest_Y)\n",
    "      model_eval_train, model_eval_test,model_eval_train_y,model_eval_test_y, cf_matrix_train, cf_matrix_test = model_eval_data(clf1, newtrain_X, newtrain_Y,\n",
    "                                                         newtest_X, newtest_Y,\n",
    "                                                         model_eval_train,\n",
    "                                                         model_eval_test,\n",
    "                                                         model_eval_train_y,\n",
    "                                                         model_eval_test_y,\n",
    "                                                         saveStep1,\n",
    "                                                         Name=name_res2+'-'+name)\n",
    "      saveStep1=0\n",
    "\n",
    "        #print(\"classifier: \",name,\"  - %s seconds -\" % (time.time() - start_time))\n",
    "      make_confusion_matrix_chart(cf_matrix_train, cf_matrix_test,name_res2+'-'+name)\n",
    "\n",
    "print('#####################Train#####################')\n",
    "df_train = pd.DataFrame.from_dict(model_eval_train)\n",
    "df_train.to_csv(NewAdd+'Train.csv')\n",
    "\n",
    "df_train1 = pd.DataFrame.from_dict(model_eval_train_y)\n",
    "df_train1.to_json(NewAdd + 'LabelPredictTrain.json')\n",
    "\n",
    "print('#####################Test#####################')\n",
    "df_test = pd.DataFrame.from_dict(model_eval_test)\n",
    "df_test.to_csv(NewAdd+'Test.csv')\n",
    "\n",
    "df_test1 = pd.DataFrame.from_dict(model_eval_test_y)\n",
    "df_test1.to_json(NewAdd + 'LabelPredictTest.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1ft2mNfSuXsR4csyFBum8Drw-3f1HDzX5"
    },
    "executionInfo": {
     "elapsed": 8923,
     "status": "ok",
     "timestamp": 1740424572189,
     "user": {
      "displayName": "Ali Ghanbari",
      "userId": "17860187318684898656"
     },
     "user_tz": -210
    },
    "id": "57Wb9yGXWjFs",
    "outputId": "800354cc-9f41-49e2-b764-45ea9a474b59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def showROC(dfResult,NameFig,name_method,i_check):\n",
    "  plt.figure()\n",
    "  fig,axs=plt.subplots(1,2,figsize=(12,5))\n",
    "  y_real=[]\n",
    "  LegNameRoc=[]\n",
    "  LegNamePRC=[]\n",
    "  t=0\n",
    "  if i_check==0: i_1,i_2=0,1\n",
    "  if i_check==1: i_1,i_2=1,0\n",
    "  for Name, Values in dfResult.items():\n",
    "    if Name=='real':\n",
    "      y_real=Values\n",
    "      continue\n",
    "\n",
    "    if (Name.split('-')[i_1]==name_method):\n",
    "      y_pred=Values\n",
    "      fpr, tpr, _ = roc_curve(y_real, y_pred)\n",
    "      Precision1,recall1,_=precision_recall_curve(y_real, y_pred)\n",
    "      f1score=f1_score(y_real, y_pred)\n",
    "      roc_auc = auc(fpr, tpr)\n",
    "      axs[0].plot(fpr, tpr)\n",
    "      axs[1].plot(Precision1,recall1)\n",
    "      LegNameRoc.append(Name.split('-')[i_2]+' ('+ str(roc_auc*100)[0:5]+' )')\n",
    "      LegNamePRC.append(Name.split('-')[i_2]+' ('+ str(f1score*100)[0:5]+' )')\n",
    "  axs[0].set_xlabel('False Positive Rate')\n",
    "  axs[0].set_ylabel('True Positive Rate')\n",
    "  axs[0].set_title('ROC Curve And AUC Of '+ name_method)\n",
    "  axs[0].legend(LegNameRoc,loc='lower right')\n",
    "\n",
    "  axs[1].set_xlabel('Recall')\n",
    "  axs[1].set_ylabel('Precision')\n",
    "  axs[1].set_title('Precision-Recall Curve And F1-Score Of '+ name_method)\n",
    "  axs[1].legend(LegNamePRC,loc='lower left')\n",
    "  plt.savefig(NewAdd+name_method+'-'+NameFig+'.png')\n",
    "  plt.show()\n",
    "\n",
    "#dfResult_Test=pd.read_csv(NewAdd+'LabelPredictTest.csv')\n",
    "jsonResult_Test=pd.read_json(NewAdd+'LabelPredictTest.json')\n",
    "jsonResult_Train=pd.read_json(NewAdd+'LabelPredictTrain.json')\n",
    "for name_method_l1 in OLD_reSamplerName:\n",
    "  for name_method_l2 in name_method_l1:\n",
    "    showROC(jsonResult_Test.iloc[:,0],'Test',name_method_l2,0)\n",
    "    showROC(jsonResult_Train.iloc[:,0],'Train',name_method_l2,0)\n",
    "\n",
    "for name_method_l1 in names:\n",
    "    showROC(jsonResult_Test.iloc[:,0],'Test',name_method_l1,1)\n",
    "    showROC(jsonResult_Train.iloc[:,0],'Train',name_method_l1,1)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
